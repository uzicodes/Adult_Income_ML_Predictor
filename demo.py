# -*- coding: utf-8 -*-
"""demo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DXBiy2GYrQ1j1tQ9C_XlERWns92xvDFU
"""

import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from sklearn.metrics import precision_score
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (classification_report, accuracy_score, confusion_matrix, precision_score, recall_score, f1_score)

"""Display the Dataset in Pandas Dataframe"""

hcv_dataset = pd.read_csv('hcvdat0.csv')
hcv_dataset.head(615)

print(hcv_dataset.shape)

hcv_dataset.isnull().sum()

plt.figure(figsize=(12, 8))
cardinal_attributes = hcv_dataset.select_dtypes(include=np.number).columns
correlation_matrix = hcv_dataset[cardinal_attributes].corr()
sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap="Blues", cbar=True, linewidths=0.5)
plt.title('Heatmap of HCV Dataset')
plt.show()

hcv_dataset["Category"].value_counts()

category_counts = hcv_dataset['Category'].value_counts()

plt.figure(figsize=(10, 6))
bars = plt.bar(category_counts.index, category_counts.values)
plt.title('Number of Instances of each Category Class')
plt.xlabel('Category')
plt.ylabel('Number of Instances')
for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height, str(height), ha='center', va='bottom')

plt.show()

updated_hcv_dataset = hcv_dataset.drop(columns = ['Unnamed: 0'], inplace = False)
updated_hcv_dataset.isnull().sum()

impute = SimpleImputer(missing_values=np.nan, strategy='median')
impute.fit(updated_hcv_dataset[['ALB']])
updated_hcv_dataset[['ALB']] = impute.transform(updated_hcv_dataset[['ALB']])

impute.fit(updated_hcv_dataset[['ALP']])
updated_hcv_dataset[['ALP']] = impute.transform(updated_hcv_dataset[['ALP']])

impute.fit(updated_hcv_dataset[['ALT']])
updated_hcv_dataset[['ALT']] = impute.transform(updated_hcv_dataset[['ALT']])

impute.fit(updated_hcv_dataset[['CHOL']])
updated_hcv_dataset[['CHOL']] = impute.transform(updated_hcv_dataset[['CHOL']])

impute.fit(updated_hcv_dataset[['PROT']])
updated_hcv_dataset[['PROT']] = impute.transform(updated_hcv_dataset[['PROT']])

updated_hcv_dataset.isnull().sum()

updated_hcv_dataset['Encoded Sex'] = 0
updated_hcv_dataset.loc[updated_hcv_dataset['Sex'] == 'm', 'Encoded Sex'] = 10
updated_hcv_dataset.loc[updated_hcv_dataset['Sex'] == 'f', 'Encoded Sex'] = 1

updated_hcv_dataset.head()

updated_hcv_dataset["Encoded Category"] = updated_hcv_dataset["Category"].copy()
for data_index in updated_hcv_dataset.index:
    category = updated_hcv_dataset.loc[data_index, "Category"]
    if category == '0=Blood Donor':
        updated_hcv_dataset.loc[data_index, "Encoded Category"] = 0
    elif category == '0s=suspect Blood Donor':
        updated_hcv_dataset.loc[data_index, "Encoded Category"] = 1
    elif category == '1=Hepatitis':
        updated_hcv_dataset.loc[data_index, "Encoded Category"] = 2
    elif category == '2=Fibrosis':
        updated_hcv_dataset.loc[data_index, "Encoded Category"] = 3
    else:
        updated_hcv_dataset.loc[data_index, "Encoded Category"] = 4

updated_hcv_dataset.head()

features_to_scale = ['ALB', 'ALP', 'ALT', 'AST', 'BIL', 'CHE', 'CHOL', 'CREA', 'GGT', 'PROT']
standard_transformer = StandardScaler()
updated_hcv_dataset[features_to_scale] = standard_transformer.fit_transform(updated_hcv_dataset[features_to_scale])
display(updated_hcv_dataset)

updated_hcv_dataset['Encoded Category'] = pd.Categorical(updated_hcv_dataset['Encoded Category'])
updated_hcv_dataset['Encoded Category'].head()
X = updated_hcv_dataset[['Age', 'ALB', 'ALP', 'ALT', 'AST', 'BIL', 'CHE', 'CHOL', 'CREA', 'GGT', 'PROT', 'Encoded Sex']]
y = updated_hcv_dataset['Encoded Category']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify = y)

decision_tree = DecisionTreeClassifier()
decision_tree.fit(X_train, y_train)

decision_tree_predictions = decision_tree.predict(X_test)
decision_tree_accuracy = accuracy_score(y_test, decision_tree_predictions)
decision_tree_precision = precision_score(y_test, decision_tree_predictions, average='weighted', zero_division=1)
decision_tree_recall = recall_score(y_test, decision_tree_predictions, average='weighted')
decision_tree_F1 = f1_score(y_test, decision_tree_predictions, average='weighted')

print("Decision Tree Accuracy:", decision_tree_accuracy)
print("Decision Tree Classification Report:\n", classification_report(y_test, decision_tree_predictions, zero_division=1))
print("Decision Tree Weighted Average Precision:", decision_tree_precision)
print("Decision Tree Weighted Average Recall:", decision_tree_recall)
print("Decision Tree Weighted Average F1 Score:", decision_tree_F1)

cm = confusion_matrix(y_test, decision_tree_predictions, normalize='true')
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=decision_tree.classes_)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix for Decision Tree Model")
plt.show()

logistic_regression = LogisticRegression(max_iter=10000000000)
logistic_regression.fit(X_train, y_train)

logistic_regression_predictions = logistic_regression.predict(X_test)

logistic_regression_accuracy = accuracy_score(y_test, logistic_regression_predictions)
logistic_regression_precision = precision_score(y_test, logistic_regression_predictions, average='weighted')
logistic_regression_recall = recall_score(y_test, logistic_regression_predictions, average='weighted')
logistic_regression_F1 = f1_score(y_test, logistic_regression_predictions, average='weighted')

print("Logistic Regression Accuracy:", logistic_regression_accuracy)
print("Logistic Regression Classification Report:\n", classification_report(y_test, logistic_regression_predictions, zero_division=1))
print("Logistic Regression Weighted Average Precision:", logistic_regression_precision)
print("Logistic Regression Weighted Average Recall:", logistic_regression_recall)
print("Logistic Regression Weighted Average F1 Score:", logistic_regression_F1)

cm = confusion_matrix(y_test, logistic_regression_predictions, normalize='true')
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=logistic_regression.classes_)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix for Logistic Regression Model")
plt.show()

naive_bayes = GaussianNB()
naive_bayes.fit(X_train, y_train)

naive_bayes_predictions = naive_bayes.predict(X_test)

naive_bayes_accuracy = accuracy_score(y_test, naive_bayes_predictions)
naive_bayes_precision = precision_score(y_test, naive_bayes_predictions, average='weighted')
naive_bayes_recall = recall_score(y_test, naive_bayes_predictions, average='weighted')
naive_bayes_F1 = f1_score(y_test, naive_bayes_predictions, average='weighted')

print("Naive Bayes Accuracy:", naive_bayes_accuracy)
print("Naive Bayes Classification Report:\n", classification_report(y_test, naive_bayes_predictions, zero_division=1))
print("Naive Bayes Weighted Average Precision:", naive_bayes_precision)
print("Naive Bayes Weighted Average Recall:", naive_bayes_recall)
print("Naive Bayes Weighted Average F1 Score:", naive_bayes_F1)

cm = confusion_matrix(y_test, naive_bayes_predictions, normalize='true')
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=naive_bayes.classes_)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix for Naive Bayes Model")
plt.show()

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)

knn_predictions = knn.predict(X_test)

knn_accuracy = accuracy_score(y_test, knn_predictions)
knn_precision = precision_score(y_test, knn_predictions, average='weighted', zero_division=1)
knn_recall = recall_score(y_test, knn_predictions, average='weighted')
knn_F1 = f1_score(y_test, knn_predictions, average='weighted')

print("KNN Accuracy:", knn_accuracy)
print("KNN Classification Report:\n", classification_report(y_test, knn_predictions, zero_division=1))
print("KNN Weighted Average Precision:", knn_precision)
print("KNN Weighted Average Recall:", knn_recall)
print("KNN Weighted Average F1 Score:", knn_F1)

cm = confusion_matrix(y_test, knn_predictions, normalize='true')
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=knn.classes_)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix for KNN Model")
plt.show()

models = ['Decision Tree', 'Logistic Regression', 'Naive Bayes', 'KNN']
accuracy_scores = [decision_tree_accuracy, logistic_regression_accuracy, naive_bayes_accuracy, knn_accuracy]
precision_scores = [decision_tree_precision, logistic_regression_precision, naive_bayes_precision, knn_precision]
recall_scores = [decision_tree_recall, logistic_regression_recall, naive_bayes_recall, knn_recall]
f1_scores = [decision_tree_F1, logistic_regression_F1, naive_bayes_F1, knn_F1]

x = np.arange(len(models))
width = 0.2

fig, ax = plt.subplots(figsize=(10, 6))
rects1 = ax.bar(x - 1.5*width, accuracy_scores, width, label='Accuracy')
rects2 = ax.bar(x - 0.5*width, precision_scores, width, label='Precision')
rects3 = ax.bar(x + 0.5*width, recall_scores, width, label='Recall')
rects4 = ax.bar(x + 1.5*width, f1_scores, width, label='F1 Score')

ax.set_ylabel('Scores')
ax.set_title('Comparison of Model Performance')
ax.set_xticks(x)
ax.set_xticklabels(models)
ax.legend(loc='lower right')

def autolabel(rects):
    for rect in rects:
        height = rect.get_height()
        ax.annotate(f'{height:.3f}', xy=(rect.get_x() + rect.get_width() / 2, height), xytext=(0, 3), textcoords="offset points", ha='center', va='bottom')

autolabel(rects1)
autolabel(rects2)
autolabel(rects3)
autolabel(rects4)

fig.tight_layout()
plt.show()

score_types = ['Accuracy', 'Precision', 'Recall', 'F1']

fig, ax = plt.subplots(figsize=(10, 6))
x = np.arange(len(score_types))
width = 0.2

rects1 = ax.bar(x - width, [decision_tree_accuracy, decision_tree_precision, decision_tree_recall, decision_tree_F1], width, label='Decision Tree')
rects2 = ax.bar(x, [logistic_regression_accuracy, logistic_regression_precision, logistic_regression_recall, logistic_regression_F1], width, label='Logistic Regression')
rects3 = ax.bar(x + width, [naive_bayes_accuracy, naive_bayes_precision, naive_bayes_recall, naive_bayes_F1], width, label='Naive Bayes')
rects4 = ax.bar(x + 2*width, [knn_accuracy, knn_precision, knn_recall, knn_F1], width, label='KNN')

ax.set_ylabel('Scores')
ax.set_title('Comparison of Metric Types')
ax.set_xticks(x)
ax.set_xticklabels(score_types)
ax.legend(loc='lower right')

for rects in [rects1, rects2, rects3, rects4]:
    autolabel(rects)

fig.tight_layout()
plt.show()

models = {'Decision Tree': (decision_tree, decision_tree_predictions), 'Logistic Regression': (logistic_regression, logistic_regression_predictions), 'Naive Bayes': (naive_bayes, naive_bayes_predictions), 'KNN': (knn, knn_predictions)}

fig, axes = plt.subplots(2, 2, figsize=(12, 10))
axes = axes.ravel()

for i, (name, (model, pred)) in enumerate(models.items()):
    cm = confusion_matrix(y_test, pred, normalize='true')
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    disp.plot(ax=axes[i], xticks_rotation=45, cmap=plt.cm.Blues)
    axes[i].set_title(f"Confusion Matrix of {name} Model")

plt.tight_layout()
plt.show()